{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation using finite-differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from scipy.integrate import quad\n",
    "sns.set_context(\"talk\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"whitegrid\")\n",
    "import sympy as sp\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "# Don't tinker, or do\n",
    "#%matplotlib nbagg\n",
    "# from matplotlib import rcParams\n",
    "#rcParams['font.family']='sans-serif' \n",
    "#rcParams('font', serif='Helvetica Neue') \n",
    "# rcParams['text.usetex']= True \n",
    "#rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical differentiation schemes\n",
    "Differentiation is another fundamental operation in calculus, and involves finding the slope of a certain curve (geometrically) at some point. This corresponds to the rate of change (or expected growth or decay of the function). We look at some schemes/algorithms to do estimate the slope, similar to our time-stepper lecture. The problem we deal with is then to approximate\n",
    "$$  \\frac{df}{dx}(x) \\approx \\sum_{i=1}^{n} \\omega_i f(x_i) $$\n",
    "given at least a \\( C^{1} \\) function $f : [a,b] \\to \\mathbb{R}$ with *nodes* ($x_i$) and *weights* ($\\omega_i$). Schemes that we will be discussing involve careful choices of these nodes and weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let us define a class that helps us implement any quadrature rule easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDifferentiator(object):\n",
    "    \"\"\" Class for wrapping a diff scheme with other goodies\n",
    "    \"\"\"\n",
    "    def __init__(self, i_a, i_b, i_h):\n",
    "        \"\"\" Initialize the Differentiator\n",
    "        \"\"\"\n",
    "        # What forcing function are we using?\n",
    "        self.forcing = None\n",
    "\n",
    "        # What differentation algorithm are we using?\n",
    "        self.differentiator = None\n",
    "\n",
    "        self.a = i_a\n",
    "        self.b = i_b\n",
    "        self.h = i_h\n",
    "        self.nsteps = int((i_b - i_a)/i_h)\n",
    "\n",
    "    def set_forcing_function(self, t_func):\n",
    "        \"\"\" Set forcing function to be used \n",
    "        \"\"\"\n",
    "        if type(t_func) is not str:\n",
    "            try:\n",
    "                # If not string, try and evaluate the function\n",
    "                t_func(0.0)\n",
    "                # If the function works, set this function as forcing\n",
    "                self.forcing = t_func\n",
    "            except:\n",
    "                raise RuntimeError('Provided function cannot be evaluated')\n",
    "        \n",
    "    def analytic(self, x):\n",
    "        \"\"\"  For testing convergence, defined as a special function \"\"\"\n",
    "        analytical_differential = (self.forcing(x)[1])\n",
    "        return analytical_differential\n",
    "            \n",
    "    def diff_using(self, differentiator, renderer):\n",
    "        \"\"\"  Differentiates the function and spits out the relative error\n",
    "        \"\"\"\n",
    "        self.differentiator = differentiator.__name__\n",
    "        # Periodic domain, b not included\n",
    "        all_x = np.linspace(self.a, self.b, self.nsteps, endpoint=False)\n",
    "        differential = differentiator(self.forcing, all_x, self.h)\n",
    "        \n",
    "        analytic_differential = self.analytic(all_x)\n",
    "        # print(\"Integral is {:20.16f}, Analytical integral is {:20.16f}, Error is {:20.16f}\".format(integral, analytical_integral, np.abs(integral - analytical_integral)))\n",
    "\n",
    "        # Pass as (2xn) arrays. While reshaping do T to get (nx2) arrays\n",
    "        self.draw(renderer, np.hstack((all_x, [self.b])), np.hstack((differential, differential[0])))\n",
    "        err = np.max(np.abs((analytic_differential - differential))) / np.max(np.abs(analytic_differential))\n",
    "        return err\n",
    "\n",
    "    def draw(self, renderer, eval_x, num_diff):\n",
    "        \"\"\" Draw the matplotlib canvas with the portrait we want\n",
    "        \"\"\"\n",
    "        # If there is a timestepper, then there is numerical data\n",
    "        # Plot them\n",
    "        renderer.plot(eval_x, self.forcing(eval_x)[0], 'g-', label=r'$f(x)$')\n",
    "        renderer.plot(eval_x, self.forcing(eval_x)[1], 'r-', label=r'$\\frac{df(x)}{dx}$') \n",
    "        renderer.plot(eval_x, num_diff, label=r'Numerical')\n",
    "        renderer.legend()\n",
    "        renderer.set_xlabel(r'$x$')\n",
    "        renderer.set_ylabel(r'$y$')\n",
    "        renderer.set_title('{}'.format(self.differentiator))\n",
    "        # renderer.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite-difference differentiation\n",
    "\n",
    "A variety of rules for differentiation exist, obtained by mainpulating the Taylor series expansion around the point of interest. We look to approximate the slope (first order of diff) and the curvature (second order of differentiation) using many types of schemes: (a) Forward (b) Backward and (c) Central differences, although others (spectral) will be discussed on the way. We will attempt to compare these methods in terms of their ease (in understanding/implementation), order of accuracy (in comparison to the discretization $h$) and function evaluations for each step $h$.\n",
    "\n",
    "## What do these rules do?\n",
    "They approximate the slope of the curve in a smart way---using local polynomials (in the case of finite-differences), you approximate the curve and differentiate these polynomials instead. We'll first look at some  schemes for obtaining first order derivatives.\n",
    "\n",
    "We first look at two point derivatives (i.e. only two function evaluations to estimate the derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdiff_oneorder_twopoint(func, x, h):\n",
    "    \"\"\"Does forward differences to approximate the slope\n",
    "    in a periodic domain\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func : ufunc\n",
    "        A function, f, that takes one argument and\n",
    "        returns the function value and exact differential\n",
    "        as a tuple\n",
    "    x : float/array-like\n",
    "        Points at which the function is sampled\n",
    "    h : float/array-like\n",
    "        The spacing between nodes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    differential : float/array-like\n",
    "        Approximation of the diff using quadrature\n",
    "    \"\"\"\n",
    "    # Evaluate function\n",
    "    fx = func(x)[0]\n",
    "    \n",
    "    # Periodic domain\n",
    "    # (f(i+1)-f(i))/h\n",
    "    df_dx = (np.roll(fx, -1) - fx) / (h)\n",
    "\n",
    "    return df_dx\n",
    "\n",
    "def bdiff_oneorder_twopoint(func, x, h):\n",
    "    \"\"\"Does backward differences to approximate the slope\n",
    "    in a periodic domain\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func : ufunc\n",
    "        A function, f, that takes one argument and\n",
    "        returns the function value and exact differential\n",
    "        as a tuple\n",
    "    x : float/array-like\n",
    "        Points at which the function is sampled\n",
    "    h : float/array-like\n",
    "        The spacing between nodes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    differential : float/array-like\n",
    "        Approximation of the diff using quadrature\n",
    "    \"\"\"\n",
    "    # Evaluate function\n",
    "    fx = func(x)[0]\n",
    "    \n",
    "    # Periodic domain\n",
    "    # (f(i)-f(i-1))/h\n",
    "    df_dx = (fx - np.roll(fx, 1)) / (h)\n",
    "\n",
    "    return df_dx\n",
    "\n",
    "def cdiff_oneorder_twopoint(func, x, h):\n",
    "    \"\"\"Does centered differences to approximate the slope\n",
    "    in a periodic domain\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func : ufunc\n",
    "        A function, f, that takes one argument and\n",
    "        returns the function value and exact differential\n",
    "        as a tuple\n",
    "    x : float/array-like\n",
    "        Points at which the function is sampled\n",
    "    h : float/array-like\n",
    "        The spacing between nodes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    differential : float/array-like\n",
    "        Approximation of the diff using quadrature\n",
    "    \"\"\"\n",
    "    # Evaluate function\n",
    "    fx = func(x)[0]\n",
    "    \n",
    "    # Periodic domain\n",
    "    # (f(i+1)-f(i-1))/2h    \n",
    "    df_dx = (np.roll(fx, -1) - np.roll(fx, 1)) / (2*h)\n",
    "\n",
    "    return df_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing diff?\n",
    "Let's test these rules out between $[a,b]$ for simple periodic functions (trigonometric) curves and draw inferences from them (the error and so on...). The choice of periodic domain is not arbitrary---we do not need to worry about boundary conditions and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds of function definition\n",
    "a = 0\n",
    "b = 1\n",
    "\n",
    "# Step size of differentiation - change\n",
    "# h = (b-a)/1\n",
    "# h = (b-a)/20\n",
    "# h = (b-a)/50\n",
    "h = (b-a)/100\n",
    "\n",
    "# Define the periodic function to be differentiated and\n",
    "# its exact differential - change\n",
    "def periodic_func(x):\n",
    "#     # Periodic function\n",
    "#     return np.sin(2.*np.pi*x), 2.*np.pi*np.cos(2.*np.pi*x)\n",
    "\n",
    "    \n",
    "    # Periodic function\n",
    "    k = 5\n",
    "    return np.sin(2.*np.pi*k*x), 2.*np.pi*k*np.cos(2.*np.pi*k*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10, 10))\n",
    "si = SpatialDifferentiator(a, b, h)\n",
    "si.set_forcing_function(periodic_func)\n",
    "\n",
    "# Differnt schemes, change\n",
    "# si.diff_using(fdiff_oneorder_twopoint, ax)\n",
    "# si.diff_using(bdiff_oneorder_twopoint, ax)\n",
    "si.diff_using(cdiff_oneorder_twopoint, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to observe\n",
    "- How does the spacing $h$ affect the derivative (visually)\n",
    "- What about the choice of function?\n",
    "- How do the central/backward/forward schemes compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher order approximations\n",
    "We then look at six point schemes to estimate the first derivative (i.e. six function evaluations to estimate the derivative). Look at https://en.wikipedia.org/wiki/Finite_difference_coefficient or http://web.media.mit.edu/~crtaylor/calculator.html for obtaining the coefficients in such schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdiff_oneorder_sixpoint(func, x, h):\n",
    "    # Evaluate function\n",
    "    fx = func(x)[0]\n",
    "    \n",
    "    # Periodic domain\n",
    "    df_dx = (-137.*fx + 300.*np.roll(fx, -1) - 300.*np.roll(fx, -2) + 200.*np.roll(fx, -3) - 75.*np.roll(fx, -4) + 12.*np.roll(fx, -5)) / (60.*h)\n",
    "    \n",
    "    return df_dx\n",
    "\n",
    "def bdiff_oneorder_sixpoint(func, x, h):\n",
    "    # Evaluate function\n",
    "    fx = func(x)[0]\n",
    "    \n",
    "    # Periodic domain\n",
    "    df_dx = (137.*fx - 300.*np.roll(fx, 1) + 300.*np.roll(fx, 2) - 200.*np.roll(fx, 3) + 75.*np.roll(fx, 4) - 12.*np.roll(fx, 5)) / (60.*h)\n",
    "    \n",
    "    return df_dx\n",
    "\n",
    "def cdiff_oneorder_sixpoint(func, x, h):\n",
    "    # Evaluate function\n",
    "    fx = func(x)[0]\n",
    "    \n",
    "    # Periodic domain\n",
    "    df_dx = (-np.roll(fx, 3) + 9. * np.roll(fx, 2) - 45. * np.roll(fx, 1) + 45. * np.roll(fx, -1) - 9. * np.roll(fx, -2) + np.roll(fx, -3) ) / (60.0 * h)\n",
    "   \n",
    "    return df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds of function definition\n",
    "a = 0\n",
    "b = 1\n",
    "\n",
    "# Step size of differentiation\n",
    "h = (b-a)/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10, 10))\n",
    "si = SpatialDifferentiator(a, b, h)\n",
    "si.set_forcing_function(periodic_func)\n",
    "si.diff_using(cdiff_oneorder_sixpoint, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to observe\n",
    "- How does the relative error change when more point stencils are used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to approximate derivatives\n",
    "\n",
    "Since we live in a periodic domain, we can spectrally differentiate functions using Fourier transforms. This is just for demonstration, and will not be covered in detail in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_oneorder(func, x, h):\n",
    "    # Evaluate function\n",
    "    fx = func(x)[0]\n",
    "    f_hat = np.fft.fft(fx)\n",
    "\n",
    "    n_samples = x.shape[0]\n",
    "    w_hat = 1j*np.zeros(n_samples)\n",
    "    k_prefac = 2*np.pi/(n_samples*h)\n",
    "    # 0 represents 0, then postive and then negative\n",
    "    w_hat[:n_samples//2] = 1j*np.arange(0, n_samples//2, 1)*k_prefac\n",
    "    # Middle one is zero, so leave as is\n",
    "    w_hat[n_samples//2:] = 1j*np.arange(-n_samples//2, 0, 1)*k_prefac\n",
    "\n",
    "    df_dx_hat = w_hat*f_hat\n",
    "    df_dx = np.real(np.fft.ifft(df_dx_hat))\n",
    "    \n",
    "    return df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds and step size\n",
    "a = 0\n",
    "b = 1\n",
    "h = (b-a)/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10, 10))\n",
    "si = SpatialDifferentiator(a, b, h)\n",
    "si.set_forcing_function(periodic_func)\n",
    "si.diff_using(spectral_oneorder, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't that awesome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of accuracy of finite-difference schemes\n",
    "What's **order** of accuracy? Order of accuracy quantifies the rate of convergence of a numerical approximation of a differential to the exact differential.\n",
    "The numerical solution ${u}$ is said to be $n^{\\text{th}}$-order accurate if the error, $e(h):=\\lVert\\tilde{{u}}-{u} \\rVert$ is proportional to the discretization-size $ h $, to the $n^{\\text{th}}$ power. That is\n",
    "\n",
    "$$ e(h)=\\lVert\\tilde{{u}}-{u} \\rVert\\leq C(h)^{n} $$\n",
    "\n",
    "Details of this are given in the slides. Here, we focus on differentiating a periodic function and figure out the order of convergence. The model problem that we deal with is to find\n",
    "$$ y = \\frac{d \\sin(2\\pi kx)}{dx} $$\n",
    "which as we know has the analytical solution $ \\tilde{y} = 2\\pi k \\cdot \\cos(2\\pi kx) $, and so error can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 1\n",
    "\n",
    "# All functions that you coded up\n",
    "impl_list = [fdiff_oneorder_twopoint, bdiff_oneorder_twopoint, cdiff_oneorder_twopoint, fdiff_oneorder_sixpoint, bdiff_oneorder_sixpoint, cdiff_oneorder_sixpoint, spectral_oneorder]\n",
    "# impl_list = [fdiff_oneorder_twopoint, bdiff_oneorder_twopoint, cdiff_oneorder_twopoint]\n",
    "# impl_list = [fdiff_oneorder_twopoint, bdiff_oneorder_twopoint, cdiff_oneorder_twopoint, fdiff_oneorder_sixpoint, bdiff_oneorder_sixpoint, cdiff_oneorder_sixpoint]\n",
    "\n",
    "# Time steps and associated errors from 2^0 to 2^(-10)\n",
    "h_steps = np.arange(1, 12, dtype=np.int16)\n",
    "errors_list = [[None for i in h_steps] for impl in impl_list]\n",
    "\n",
    "# Run simulations and collect errors\n",
    "for i_impl, impl in enumerate(impl_list):\n",
    "    for i_step, step in enumerate(h_steps):\n",
    "        new_h = (b-a)/(8*(step))\n",
    "        si = SpatialDifferentiator(a, b, new_h)\n",
    "        si.set_forcing_function(periodic_func)\n",
    "        errors_list[i_impl][i_step] = si.diff_using(impl, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw error plots in a log-log plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 10))\n",
    "\n",
    "# x axis is time, y axis is error\n",
    "for i_impl, impl in enumerate(impl_list):\n",
    "    ax.plot((b-a)/(8.*h_steps), errors_list[i_impl], 'o-', label=impl.__name__)\n",
    "\n",
    "# Draw helpful slope lines to compare\n",
    "# x_ax = (b-a)/(8.*h_steps)\n",
    "# ax.plot(x_ax, 1.2 * x_ax**2, 'k--')\n",
    "# ax.plot(x_ax, 0.5 * x_ax**2, 'k--')\n",
    "# ax.plot(x_ax, 0.015 * x_ax**4, 'k--')  \n",
    "\n",
    "# Make it readable\n",
    "ax.set_xlabel(r'$h$')\n",
    "ax.set_ylabel(r'$e(h)$')\n",
    "ax.set_title('Order of accuracy')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best')\n",
    "# fig.savefig('ooa.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0, 4.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplusone = np.roll(a, -1)\n",
    "aplusone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.roll(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.pad(a, (2,2), 'constant', constant_values=(10,15))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
